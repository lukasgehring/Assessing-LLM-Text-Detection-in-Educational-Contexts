2025-02-24 13:31:37.031 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:31:37.031 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250130074608', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 31, 36, 645305), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:31:37.032 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:31:37.064 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250130074608" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:31:37.064 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:31:37.068 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: 67d8cb175525780e6c932662d223089c1b9ea7e8755b96bfd31e547eccfb1b19 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:31:44.659 | INFO     | Results preview: RoBERTa ROC AUC: 1.0, PR AUC: 1.0 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:31:44.667 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250130074608/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:31:51.898 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:31:51.899 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250206132710', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 31, 51, 576679), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:31:51.899 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:31:51.926 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250206132710" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:31:51.926 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:31:51.930 | INFO     | Human data hash: e1ff295865269b93d2956ecb98e0311dab1f63e01052b81dc6e3c2f5d6e7b49d | LLM data hash: 385511ce3bbd0ce969688f823fdf29244149fc3ceb4c5ac57e4445022f5533c5 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:31:58.305 | INFO     | Results preview: RoBERTa ROC AUC: 0.9999791145981997, PR AUC: 0.9999790868746872 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:31:58.313 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250206132710/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:32:05.492 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:32:05.493 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250206132753', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 32, 5, 154697), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:32:05.493 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:32:05.519 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250206132753" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:32:05.520 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:32:05.524 | INFO     | Human data hash: e1ff295865269b93d2956ecb98e0311dab1f63e01052b81dc6e3c2f5d6e7b49d | LLM data hash: 4e63894b2e024e611c61e4a3170ac44a5a7ae17681ba698331ad1e25da6f4775 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:32:11.864 | INFO     | Results preview: RoBERTa ROC AUC: 0.9999930381993999, PR AUC: 0.9999930473596638 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:32:11.871 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250206132753/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:32:19.066 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:32:19.066 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250206132923', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 32, 18, 742620), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:32:19.066 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:32:19.097 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250206132923" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:32:19.097 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:32:19.101 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: d68a3e85591502eff7aabbf684af8083dba1d56b67c65c118a7e1852c2ecc7af (utils.args:add_data_hash_to_args:16)
2025-02-24 13:32:25.684 | INFO     | Results preview: RoBERTa ROC AUC: 0.9999938120343556, PR AUC: 0.9999938197117323 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:32:25.692 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250206132923/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:32:32.896 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:32:32.896 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250206132946', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 32, 32, 561864), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:32:32.897 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:32:32.930 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250206132946" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:32:32.930 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:32:32.934 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: 3f0278c444c7b3639e10908e2ac90fb9ef8ed9d1a9d3f61887f9a2804484ca4c (utils.args:add_data_hash_to_args:16)
2025-02-24 13:32:39.746 | INFO     | Results preview: RoBERTa ROC AUC: 0.999715353580357, PR AUC: 0.9996993137466025 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:32:39.755 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250206132946/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:32:46.908 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:32:46.908 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/gpt-4o-mini-2024-07-18/20250220090216', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 32, 46, 584581), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:32:46.908 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:32:46.939 | SUCCESS  | Loaded data: "brat-project/gpt-4o-mini-2024-07-18/20250220090216" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:32:46.940 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:32:46.944 | INFO     | Human data hash: 0d1071b0b3a2f501286655d5152a254e02f5d0dcd49aeacfd5b88814f1cf830a | LLM data hash: 67d8cb175525780e6c932662d223089c1b9ea7e8755b96bfd31e547eccfb1b19 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:32:53.601 | INFO     | Results preview: RoBERTa ROC AUC: 0.9566966164203856, PR AUC: 0.9488965493996379 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:32:53.610 | SUCCESS  | RoBERTa results saved at results/brat-project/gpt-4o-mini-2024-07-18/20250220090216/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:33:00.808 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:33:00.808 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/131538', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 33, 0, 483765), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:33:00.808 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:33:00.838 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/131538" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:33:00.838 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:33:00.841 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: 75508f1f70910a66403c4732021ee0893365dc59b24a4b661b72b22480793982 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:33:07.515 | INFO     | Results preview: RoBERTa ROC AUC: 0.999925744412267, PR AUC: 0.9999248033981599 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:33:07.524 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/131538/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:33:14.814 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:33:14.814 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/132912', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 33, 14, 486568), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:33:14.815 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:33:14.843 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/132912" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:33:14.843 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:33:14.847 | INFO     | Human data hash: e1ff295865269b93d2956ecb98e0311dab1f63e01052b81dc6e3c2f5d6e7b49d | LLM data hash: e05be616361f6b42c1eaaffd9af2214f65f7bda9d67ccf39fcb8c27747b7449e (utils.args:add_data_hash_to_args:16)
2025-02-24 13:33:21.132 | INFO     | Results preview: RoBERTa ROC AUC: 0.999930381993999, PR AUC: 0.9999296351485534 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:33:21.141 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/132912/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:33:28.386 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:33:28.386 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/132913', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 33, 28, 58512), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:33:28.386 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:33:28.411 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/132913" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:33:28.412 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:33:28.416 | INFO     | Human data hash: e1ff295865269b93d2956ecb98e0311dab1f63e01052b81dc6e3c2f5d6e7b49d | LLM data hash: 149b16b4c47eaebf7299b47bac0d7892ebc6888f02117d3ef8447f7b1c5c2ec3 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:33:34.644 | INFO     | Results preview: RoBERTa ROC AUC: 0.9998607639879978, PR AUC: 0.9998581642256729 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:33:34.653 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/132913/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:33:41.901 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:33:41.902 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/133529', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 33, 41, 567689), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:33:41.902 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:33:41.933 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/133529" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:33:41.933 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:33:41.937 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: 44afd3f3bb2a8230877089eee2b8883853e99db28576e12e6a9a8a3ca331dfc3 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:33:48.956 | INFO     | Results preview: RoBERTa ROC AUC: 0.9998948045840449, PR AUC: 0.9998958514561983 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:33:48.965 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/133529/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:33:56.185 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:33:56.186 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/133857', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 33, 55, 857757), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:33:56.186 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:33:56.217 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/133857" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:33:56.217 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:33:56.221 | INFO     | Human data hash: bde5f22229a96fe7d17e58593e1d57ef9689fa03f2132f3cef18b61267cce20b | LLM data hash: 557ff420b813ac7c68d6fca7f7fd66160d451f5a3d809867d205e76defa35fd2 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:34:03.284 | INFO     | Results preview: RoBERTa ROC AUC: 0.9995606544392465, PR AUC: 0.999546208134849 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:34:03.293 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/133857/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:34:10.538 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:34:10.539 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'brat-project/llama-3.3-70b-instruct/135465', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/ielts/llama-3.3-70b-instruct/135672/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 34, 10, 209005), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:34:10.539 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:34:10.570 | SUCCESS  | Loaded data: "brat-project/llama-3.3-70b-instruct/135465" | Used indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401] (utils.load_data:load_data:54)
2025-02-24 13:34:10.570 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:34:10.573 | INFO     | Human data hash: 55728a87a8ec7b48ae7c109900c803448c603f4b58c086632097396b958081c3 | LLM data hash: 75508f1f70910a66403c4732021ee0893365dc59b24a4b661b72b22480793982 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:34:17.276 | INFO     | Results preview: RoBERTa ROC AUC: 0.926743768718596, PR AUC: 0.9070487980371501 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:34:17.285 | SUCCESS  | RoBERTa results saved at results/brat-project/llama-3.3-70b-instruct/135465/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:34:24.497 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:34:24.497 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250220103419', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 34, 24, 164207), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:34:24.498 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:34:24.528 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250220103419" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:34:24.528 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:34:24.532 | INFO     | Human data hash: bc5daee4fdd3b55044388974edb0b1bd961aa0b3ffdd962593774b0dd7ee8aa4 | LLM data hash: 25506b72f71135df0ea77b1792d80826c2d9d5144561da7e6fdc4efac3876904 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:34:31.739 | INFO     | Results preview: RoBERTa ROC AUC: 0.7864969745213028, PR AUC: 0.7369426198337614 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:34:31.748 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250220103419/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:34:39.048 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:34:39.048 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250220103511', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 34, 38, 707350), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:34:39.048 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:34:39.080 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250220103511" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:34:39.080 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:34:39.083 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 2859e7c534bea14612c5df29b29228dd0c2eb738cd8f396dcdf10409a5dfd8b9 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:34:45.588 | INFO     | Results preview: RoBERTa ROC AUC: 0.9829292106392373, PR AUC: 0.9720322281786711 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:34:45.597 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250220103511/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:34:52.916 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:34:52.916 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250220103657', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 34, 52, 587931), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:34:52.916 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:34:52.946 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250220103657" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:34:52.946 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:34:52.950 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 25506b72f71135df0ea77b1792d80826c2d9d5144561da7e6fdc4efac3876904 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:34:59.606 | INFO     | Results preview: RoBERTa ROC AUC: 0.9900218282224613, PR AUC: 0.9855853390859085 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:34:59.615 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250220103657/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:35:06.887 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:35:06.887 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250221103134', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 35, 6, 554660), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:35:06.887 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:35:06.916 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250221103134" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:35:06.916 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:35:06.920 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 1dc0d42b6e08d963ec3e762c05bae7d98a00c57dd120f5d166c6d9a8dd775693 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:35:13.405 | INFO     | Results preview: RoBERTa ROC AUC: 0.9911754280135073, PR AUC: 0.987979896144087 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:35:13.413 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250221103134/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:35:20.621 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:35:20.621 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250221120502', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 35, 20, 296867), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:35:20.621 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:35:20.648 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250221120502" | Used indices: [1395, 845, 1429, 1384, 1383, 425, 1329, 1417, 1409, 1323, 1066, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1297, 1332, 1327, 955, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1316, 1359, 1432, 701, 1354, 25, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1422, 1215, 865, 765, 613, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 76, 1426, 1313, 1284, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 1229, 1433, 1387, 1396, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 397, 1412, 1377, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:35:20.648 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:35:20.652 | INFO     | Human data hash: bf99ed7ace01896d8af883cd4fe203cbb1ec7827d06c1215cd3c14731c575381 | LLM data hash: 3102f030e1b9df1a566b208a549ce05ecb22d2190e168866f2158c0f738eacc1 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:35:26.737 | INFO     | Results preview: RoBERTa ROC AUC: 0.9882382222222222, PR AUC: 0.9808128418107831 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:35:26.744 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250221120502/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:35:33.948 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:35:33.948 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/gpt-4o-mini-2024-07-18/20250221120538', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 35, 33, 618698), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:35:33.948 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:35:33.976 | SUCCESS  | Loaded data: "ielts/gpt-4o-mini-2024-07-18/20250221120538" | Used indices: [1395, 845, 1429, 1384, 1383, 425, 1329, 1417, 1409, 1323, 1066, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1297, 1332, 1327, 955, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1316, 1359, 1432, 701, 1354, 25, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1422, 1215, 865, 765, 613, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 76, 1426, 1313, 1284, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 1229, 1433, 1387, 1396, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 397, 1412, 1377, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:35:33.976 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:35:33.980 | INFO     | Human data hash: bf99ed7ace01896d8af883cd4fe203cbb1ec7827d06c1215cd3c14731c575381 | LLM data hash: c166ad3428e2eacd15f2d0b60a6444531003e1eef1b628d2b1898307a55f7bb8 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:35:40.030 | INFO     | Results preview: RoBERTa ROC AUC: 0.9885866666666666, PR AUC: 0.9831047107945512 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:35:40.038 | SUCCESS  | RoBERTa results saved at results/ielts/gpt-4o-mini-2024-07-18/20250221120538/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:35:47.248 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:35:47.249 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/135670', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 35, 46, 918306), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:35:47.249 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:35:47.278 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/135670" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:35:47.278 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:35:47.282 | INFO     | Human data hash: 94308f89bb275108167171021c4766707e380097acf9e57f5b5be00b0d0b0dd5 | LLM data hash: 4bc5cd1c3b0e10075f4943ac4d3042ab3010123a0dd632ebd7e1b92a83ca74c7 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:35:53.849 | INFO     | Results preview: RoBERTa ROC AUC: 0.8961449244718628, PR AUC: 0.8849102438892045 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:35:53.858 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/135670/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:36:01.016 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:36:01.016 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/135671', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 36, 0, 691842), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:36:01.017 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:36:01.046 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/135671" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:36:01.046 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:36:01.050 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 7334ac2512949966b7f15bbfcee1bf1de86ede45a81949a70812fac3e5132ad2 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:36:07.709 | INFO     | Results preview: RoBERTa ROC AUC: 0.9842413915336348, PR AUC: 0.9771360598357076 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:36:07.718 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/135671/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:36:14.959 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:36:14.959 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/135672', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 36, 14, 629103), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:36:14.960 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:36:14.993 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/135672" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:36:14.993 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:36:14.997 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 4bc5cd1c3b0e10075f4943ac4d3042ab3010123a0dd632ebd7e1b92a83ca74c7 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:36:21.539 | INFO     | Results preview: RoBERTa ROC AUC: 0.9930535257865312, PR AUC: 0.9913458782529297 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:36:21.550 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/135672/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:36:28.810 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:36:28.810 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/136001', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 36, 28, 484449), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:36:28.810 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:36:28.836 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/136001" | Used indices: [1395, 845, 1429, 1384, 1383, 425, 1329, 1417, 1409, 1323, 1066, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1297, 1332, 1327, 955, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1316, 1359, 1432, 701, 1354, 25, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1422, 1215, 865, 765, 613, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 76, 1426, 1313, 1284, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 1229, 1433, 1387, 1396, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 397, 1412, 1377, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:36:28.837 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:36:28.840 | INFO     | Human data hash: bf99ed7ace01896d8af883cd4fe203cbb1ec7827d06c1215cd3c14731c575381 | LLM data hash: f8e376b86d3beb8951fa148c383b013048c5bedafa4c55e6c90eda3aac967abb (utils.args:add_data_hash_to_args:16)
2025-02-24 13:36:34.880 | INFO     | Results preview: RoBERTa ROC AUC: 0.9914737777777777, PR AUC: 0.9888575972533432 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:36:34.888 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/136001/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:36:42.192 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:36:42.192 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/136002', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 36, 41, 859139), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:36:42.192 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:36:42.221 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/136002" | Used indices: [1395, 845, 1429, 1384, 1383, 425, 1329, 1417, 1409, 1323, 1066, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1297, 1332, 1327, 955, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1316, 1359, 1432, 701, 1354, 25, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1422, 1215, 865, 765, 613, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 76, 1426, 1313, 1284, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 1229, 1433, 1387, 1396, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 397, 1412, 1377, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:36:42.221 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:36:42.224 | INFO     | Human data hash: bf99ed7ace01896d8af883cd4fe203cbb1ec7827d06c1215cd3c14731c575381 | LLM data hash: 5bb3b3ec52f7668a61cfc2f65f2b8df530dba165f0de3f5ebad2cc063ca47a8c (utils.args:add_data_hash_to_args:16)
2025-02-24 13:36:48.303 | INFO     | Results preview: RoBERTa ROC AUC: 0.9916977777777778, PR AUC: 0.9887144627185865 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:36:48.312 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/136002/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
2025-02-24 13:36:55.575 | INFO     | System resources -> | CPUs: 120 on 0.00MHz | GPUs: 1 (NVIDIA A40) | RAM 442.63GB | (utils.logger:log_resources:11)
2025-02-24 13:36:55.576 | INFO     | Run using the following arguments: {'models': ['roberta'], 'dataset': 'ielts/llama-3.3-70b-instruct/136003', 'device': 'cuda:0', 'n_samples': 5, 'chunk_size': 20, 'base_model_name': 'gpt2-xl', 'mask_filling_model_name': 't5-3b', 'cache_dir': '.resources', 'max_num_attempts': 20, 'disable_log_file': False, 'openai_key': '', 'seed': 42, 'max_words': None, 'cut_sentences': False, 'use_detector_cache': False, 'checkpoint': 'detectors/RoBERTa/checkpoints/brat-project/llama-3.3-70b-instruct/131538/checkpoint-21', 'buffer_size': 1, 'mask_top_p': 1.0, 'pct_words_masked': 0.3, 'n_perturbation': 100, 'span_length': 2, 'start_at': datetime.datetime(2025, 2, 24, 13, 36, 55, 243062), 'job_id': '136720'} (__main__:<module>:67)
2025-02-24 13:36:55.576 | INFO     | Running on seed 42 (utils.seeds:set_seeds:8)
2025-02-24 13:36:55.608 | SUCCESS  | Loaded data: "ielts/llama-3.3-70b-instruct/136003" | Used indices: [1095, 1374, 1395, 845, 1429, 1364, 1366, 1384, 1383, 857, 425, 1329, 1363, 1417, 1409, 1323, 1066, 1362, 1421, 1319, 1410, 1306, 1406, 1305, 1407, 1404, 1293, 1339, 1365, 1388, 1411, 361, 671, 1343, 1320, 1331, 1318, 713, 1265, 1399, 681, 101, 1304, 785, 1334, 1344, 641, 1237, 1301, 1336, 753, 1296, 1287, 1105, 1420, 1197, 1375, 1297, 1332, 1327, 955, 1367, 1290, 1288, 789, 1345, 327, 837, 1317, 1298, 1413, 795, 1434, 1315, 1259, 1425, 1390, 1321, 1391, 1368, 1350, 1316, 1359, 1432, 701, 1354, 1351, 25, 1361, 633, 1418, 1253, 645, 1326, 1370, 709, 1286, 1264, 1402, 913, 903, 897, 1308, 1311, 1314, 267, 1, 1381, 1337, 1373, 1357, 1385, 1393, 1394, 1302, 771, 1352, 1398, 665, 11, 725, 73, 1061, 1355, 1335, 877, 379, 1428, 1300, 1369, 1322, 1273, 1342, 1376, 1057, 1289, 1294, 1073, 1292, 1285, 1405, 1423, 87, 1330, 1353, 779, 1401, 727, 337, 411, 59, 1325, 575, 1017, 1341, 1422, 1215, 865, 765, 613, 1349, 1386, 1281, 1081, 1431, 693, 1309, 1358, 949, 1389, 655, 1347, 1333, 1403, 809, 1397, 628, 109, 248, 1170, 702, 530, 960, 688, 362, 626, 1136, 272, 314, 1408, 1414, 31, 982, 664, 504, 1128, 1072, 862, 864, 96, 984, 450, 1100, 414, 556, 936, 350, 1024, 796, 810, 830, 426, 1112, 433, 496, 700, 538, 1156, 618, 298, 38, 472, 1082, 774, 586, 1038, 458, 336, 574, 606, 760, 422, 320, 126, 486, 240, 734, 844, 794, 768, 192, 966, 916, 528, 784, 136, 1360, 1291, 1307, 1042, 52, 1188, 660, 714, 78, 920, 922, 722, 568, 898, 852, 1018, 1238, 14, 972, 1258, 1280, 1182, 396, 912, 1056, 1216, 874, 1104, 638, 120, 882, 938, 825, 1299, 466, 170, 650, 654, 1278, 546, 596, 678, 24, 744, 152, 998, 560, 1372, 1242, 292, 1206, 380, 1228, 344, 264, 88, 948, 892, 1212, 1162, 1168, 48, 836, 788, 889, 112, 661, 803, 623, 76, 1426, 1313, 1284, 1217, 1427, 1324, 919, 1338, 603, 883, 1328, 1310, 861, 1229, 1433, 1387, 1396, 1356, 1348, 47, 1380, 1400, 1312, 1303, 1295, 1392, 1430, 163, 85, 469, 967, 515, 499, 1143, 1045, 479, 595, 317, 1025, 233, 987, 1033, 1117, 141, 555, 305, 1161, 1051, 939, 587, 1133, 981, 1173, 973, 495, 273, 135, 525, 323, 247, 181, 149, 1151, 221, 565, 189, 569, 1003, 531, 293, 925, 235, 115, 131, 933, 351, 1185, 343, 1199, 397, 1412, 1377, 1379, 917, 1419] (utils.load_data:load_data:54)
2025-02-24 13:36:55.608 | INFO     | Executing RoBERTa model (__main__:run:43)
2025-02-24 13:36:55.612 | INFO     | Human data hash: ba9065c41813b2c19b090dae868ccebab01a772e1ca10f96bdbacb2a8c9adf3c | LLM data hash: 8346b14dfa622f97c3989786a3813109331948f7f1fd5cf902c6223a51f4ce06 (utils.args:add_data_hash_to_args:16)
2025-02-24 13:37:02.420 | INFO     | Results preview: RoBERTa ROC AUC: 0.9911070204787283, PR AUC: 0.9880020771940717 (detectors.evaluate:run_perturbation_experiment:9)
2025-02-24 13:37:02.429 | SUCCESS  | RoBERTa results saved at results/ielts/llama-3.3-70b-instruct/136003/roberta/136720/roberta_136720.gz (utils.save_data:save_results:56)
